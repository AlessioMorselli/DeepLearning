<!DOCTYPE html>
<!--
To change this license header, choose License Headers in Project Properties.
To change this template file, choose Tools | Templates
and open the template in the editor.
-->
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Deep Learning - Inizializzazione dei parametri</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- framework - Da condividere -->
    <link href="../framework/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="../framework/js/bootstrap.min.js"></script>
    <!-- css comune -->
    <link href="../css/style.css" rel="stylesheet">
  </head>
  <body>
    <div class="container">
      <!-- Header -->
      <div id="header">
        
      </div>
      
      <script>
        $("#header").load("../shared/header.html");
      </script>
      <!-- Header - FINE -->
      
      <h1 class="centrato">Inizializzazione dei parametri</h1>
      
      <p class="centrato">Come primo approccio, si può pensare di inizializzare pesi e <i>bias</i> in maniera
        completamente casuale. Solitamente, si usano variabili di tipo <i>gaussiano</i> normalizzate,
        considerando ogni parametro indipendente l'uno dall'altro. Ma possiamo trovare un modo migliorare
        di inizializzare i nostri parametri e quindi aiutare la rete ad imparare più velocemente?</p>
      
      <div class="figura">
        <img src="../documentation/gaussian.png" style="width: 600px"><br/>
        <p><small>I valori sull'asse y rappresentano la probabilità che il parametro assuma il valore
            corrispondente sulla x. Avremo maggiori probabilità di ottenere pesi e <i>bias</i> distribuiti
            intorno allo zero.</small></p>
      </div>
      
      <p class="centrato">Usando l'approccio descritto sopra è probabile che i neuroni nascosti siano <b>saturi</b>,
        ovvero piccoli cambiamenti dei loro pesi causano solo minuscoli cambiamenti nei loro output e quindi
        hanno poco effetto sul resto della rete. Questo fa sì che questi neuroni saturi imparino molto lentamente!</p>
      
      <p class="centrato">Un buon modo per inizializzare i pesi è scegliere l'altezza della gaussiana in base
        al numero di input che il neurone riceve: questa altezza aumenta e la gaussiana si restringe all'aumentare
        del numero di qeusti input, diminuendo
        la probabilità di avere pesi alti. Così i neuroni tendono meno a saturare e la nostra rete raggiunge
        il massimo dell'accuratezza in meno tempo. Infatti, tale accorgimento non migliora l'accuratezza,
        ma solo il tempo che la rete impiega a raggiungere quella massima.</p>
      
      <div class="figura">
        <img src="../documentation/gaussian2.png" style="width: 600px"><br/>
        <p><small>Diverse forme della gaussiana a seconda del numero di input.</small></p>
      </div>

      <p class="centrato">Finora abbiamo parlato solo dei pesi. Infatti, per quanto riguardo i <i>bias</i>,
        la loro inizializzazione non ha un grande impatto sulla velocità di apprendimento della rete e
        quindi non modifichiamo il modo con cui li scegliamo.</p>

    </div>
  </body>
</html>
