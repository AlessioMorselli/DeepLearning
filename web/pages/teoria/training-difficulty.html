<!DOCTYPE html>
<!--
To change this license header, choose License Headers in Project Properties.
To change this template file, choose Tools | Templates
and open the template in the editor.
-->
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Deep Learning - Difficoltà di allenamento</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <!-- framework - Da condividere -->
    <link href="../framework/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="../framework/js/bootstrap.min.js"></script>
    <!-- css comune -->
    <link href="../css/style.css" rel="stylesheet">
    
  </head>
  <body>
    <div class="container">
      <!-- Header -->
      <div id="header">
        
      </div>
      
      <script>
        $("#header").load("../shared/header.html");
      </script>
      <!-- Header - FINE -->
      
      <h1 class="centrato">Difficoltà di allenamento</h1>
      
      <p class="centrato">Possiamo costruire reti neurali di qualsiasi complessità: dalle più semplici, composte
        da un solo <i>hidden layer</i> e con pochi neuroni, alle più complesse, con decine di neuroni nascosti.
        Ovviamente, sono le reti complesse che sono in grado di eseguire i compiti più complicati e sono perciò
        loro oggetto di studio. Infatti, più <i>hidden layer</i> possono creare diversi livelli di astrazione.</p>
      
      <p class="centrato">Tuttavia, quando si è provato ad allenare reti complesse si è notato un problema non
        indifferente: i diversi strati di neuroni in una rete complessa imparano a velocità ampiamente diverse.
        In particolare, quando gli ultimi <i>layer</i> si stanno allenando bene, i primi faticano a migliorare
        (o viceversa). Insomma, la rete neurale non sta imparando in maniera uniforme. Sembra vi sia una
        instabilità intrinseca associata all'apprendimento delle reti neurali complesse che usano l'algoritmo
        del gradiente decrescente.</p>
      
      <p class="centrato">Se analizziamo la velocità di cambiamento di pesi e <i>bias</i> all'interno di una
        rete neurale con due strati, ad esempio, non ci sorprende vedere velocità diverse, in quanto questi
        parametri sono inizializzati in maniera casuale. Tuttavia, ci si è accorti che i neuroni del secondo
        <i>layer</i> imparano più velocemente dei neuroni nel primo strato della rete.</p>
      
      <div class="figura">
        <img src="../documentation/training-speed.png" style="width: 400px"><br/>
        <p><small>La velocità di apprendimento dei due <i>layer</i>: si può vedere come il primo sia più
            lento del secondo.</small></p>
      </div>
      
      <p class="centrato">Inoltre, se si aggiungono <i>layer</i>, tale schema si ripresenta, causando un
        allenamento più veloce in fondo alla rete che non all'inizio. Questo fenomeno prende il nome di
        <b>problema del gradiente evanescente</b>. L'alternativa non è migliore: fare in modo che siano i primi
        <i>layer</i> ad imparare più velocemente rallenta quelli successivi
        (<b>problema del gradiente divergente</b>).</p>
      
      <div class="figura" style="padding: 0 10%;">
        <img src="../documentation/training-speed2.png" style="width: 400px">
        <img src="../documentation/training-speed3.png" style="width: 400px"><br/>
        <p><small>Anche aggiungendo livelli alla rete il problema non si risolve, anzi la differenza tra il
            primo e l'ultimo <i>layer</i> si fa sempre più evidente.</small></p>
      </div>
      
      <p class="centrato">Più in generale, quindi, il gradiente (che stabilisce la velocità di apprendimento
        della rete) è instabile e tende o a diventare eccessivamente piccolo o troppo grande.</p>
      
      <p class="centrato">Ma cosa lo causa esattamente? Tralasciando i calcoli, il problema è insito
        nell'algoritmo che usiamo, che sfrutta una serie di prodotti, in cui però gli elementi sono minori di 1.
        poiché questi prodotti coinvolgono sempre più elementi man mano che torniamo nei primi <i>layer</i>, il
        gradiente si abbassa sempre di più e i neuroni imparano sempre più lentamente.</p>
      
      <p class="centrato">Se modifichiamo i parametri in modo che essi siano maggiori di 1, otterremo l'effetto
        inverso e cadremo nel problema divergente. Il problema fondamentale è comunque l'<b>instabilità</b>
        della situazione. Infatti, le reti più complesse soffrono il <b>problema del gradiente instabile</b>.</p>
      
      <p class="centrato">Si sta studiando molto il problema e le sue causa e non è ancora stata trovata
        una soluzione sistematica e ben definita, bensì si procede caso per caso, basandosi sulle prestazioni
        che ottengono le nostre idee.</p>

    </div>
  </body>
</html>
